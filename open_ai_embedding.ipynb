{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa5fb3f-51fb-46a1-b599-808d5d6ebb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp310-cp310-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.16.1-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading gast-0.5.5.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading h5py-3.11.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached ml_dtypes-0.3.2-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.12.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading grpcio-1.64.1-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached optree-0.11.0-cp310-cp310-win_amd64.whl.metadata (46 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\janni\\dropbox\\projects\\2024_05 ieee suicide prevention\\ieee-2024-suicide-prevention\\venv\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.16.1-cp310-cp310-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.16.1-cp310-cp310-win_amd64.whl (376.9 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.64.1-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/4.1 MB 1.7 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/4.1 MB 1.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.2/4.1 MB 1.2 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/4.1 MB 1.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.3/4.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.3/4.1 MB 1.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.4/4.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.5/4.1 MB 1.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.5/4.1 MB 1.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.6/4.1 MB 1.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.7/4.1 MB 1.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.8/4.1 MB 1.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.0/4.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.0/4.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.2/4.1 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.3/4.1 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.4/4.1 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 1.5/4.1 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.6/4.1 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.8/4.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.9/4.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.0/4.1 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.1/4.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.3/4.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 2.4/4.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.6/4.1 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.7/4.1 MB 2.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.9/4.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.0/4.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.2/4.1 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.4/4.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.5/4.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.7/4.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.9/4.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.0/4.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.1/4.1 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading h5py-3.11.0-cp310-cp310-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/3.0 MB 4.1 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/3.0 MB 4.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.5/3.0 MB 4.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.7/3.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.9/3.0 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.1/3.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.2/3.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.4/3.0 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.6/3.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.8/3.0 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.0/3.0 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.2/3.0 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.4/3.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.6/3.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.7/3.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/3.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.4/1.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.5/1.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.7/1.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.9/1.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.1/1.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 3.6 MB/s eta 0:00:00\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp310-cp310-win_amd64.whl (127 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "   ---------------------------------------- 0.0/227.3 kB ? eta -:--:--\n",
      "   ---------------------------------------  225.3/227.3 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 227.3/227.3 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.11.0-cp310-cp310-win_amd64.whl (243 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.5.5-py3-none-any.whl size=20105 sha256=e4736172dec0442f0b43bcdc618fbf912e9e2617b4237a989a974d7c4ff23693\n",
      "  Stored in directory: c:\\users\\janni\\appdata\\local\\pip\\cache\\wheels\\3b\\8d\\9c\\b96881d3ff78880d858437bdca928012d64f322a2801188181\n",
      "Successfully built gast\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.5 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 keras-3.4.1 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 werkzeug-3.0.3 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745be55-49e6-4e14-bbd6-4f9f6ea809fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "from src import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd0d7d1-fedd-4715-9736-dbf698810829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>post_risk</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just kill me. Please! Just end my life! I beg ...</td>\n",
       "      <td>ideation</td>\n",
       "      <td>[0.05026989057660103, 0.00029446851112879813, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There is no today, nor tommorrow. I dont have ...</td>\n",
       "      <td>behavior</td>\n",
       "      <td>[-0.007263310253620148, 0.01930110529065132, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life so full of contradictions that it's not w...</td>\n",
       "      <td>indicator</td>\n",
       "      <td>[0.034987498074769974, 0.022024665027856827, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I think I'm going to kill myself soon. I don't...</td>\n",
       "      <td>behavior</td>\n",
       "      <td>[-0.005158697720617056, 0.020152874290943146, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whats the point of living. no really. is there...</td>\n",
       "      <td>ideation</td>\n",
       "      <td>[0.011075956746935844, 0.054204490035772324, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  post_risk  \\\n",
       "0  Just kill me. Please! Just end my life! I beg ...   ideation   \n",
       "1  There is no today, nor tommorrow. I dont have ...   behavior   \n",
       "2  Life so full of contradictions that it's not w...  indicator   \n",
       "3  I think I'm going to kill myself soon. I don't...   behavior   \n",
       "4  whats the point of living. no really. is there...   ideation   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.05026989057660103, 0.00029446851112879813, ...  \n",
       "1  [-0.007263310253620148, 0.01930110529065132, -...  \n",
       "2  [0.034987498074769974, 0.022024665027856827, -...  \n",
       "3  [-0.005158697720617056, 0.020152874290943146, ...  \n",
       "4  [0.011075956746935844, 0.054204490035772324, 0...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled = pd.read_parquet(paths.INTERMEDIATE_DATA_PATH / \"training_data_embedding.parquet\")\n",
    "labelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90de4532-256a-4b93-906f-70209ee2588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labelled['post_risk'].values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "embeddings = np.array(labelled['embedding'].tolist())\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, encoded_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9833c4-bb9c-4dea-ae27-4525377a0950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.478 total time=   0.7s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.454 total time=   0.6s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.440 total time=   0.5s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.442 total time=   0.5s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=50;, score=0.489 total time=   0.5s\n",
      "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.473 total time=   1.0s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.418 total time=   0.9s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.483 total time=   1.4s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.469 total time=   1.0s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=100;, score=0.485 total time=   1.2s\n",
      "[CV 1/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=0.494 total time=   2.4s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=0.471 total time=   2.5s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=0.510 total time=   2.2s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=0.469 total time=   1.9s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=2, n_estimators=200;, score=0.474 total time=   2.0s\n",
      "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.427 total time=   0.4s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.497 total time=   0.5s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.453 total time=   0.5s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.525 total time=   0.6s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=50;, score=0.435 total time=   0.6s\n",
      "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.487 total time=   1.1s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.517 total time=   1.0s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.491 total time=   0.9s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.519 total time=   1.1s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=100;, score=0.468 total time=   0.9s\n",
      "[CV 1/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=0.472 total time=   2.1s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=0.490 total time=   2.8s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=0.487 total time=   2.4s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=0.534 total time=   2.6s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=5, n_estimators=200;, score=0.429 total time=   3.3s\n",
      "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.504 total time=   0.5s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.409 total time=   0.5s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.502 total time=   0.4s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.498 total time=   0.5s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=50;, score=0.437 total time=   0.5s\n",
      "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.453 total time=   1.2s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.406 total time=   1.3s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.507 total time=   1.0s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.571 total time=   1.0s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=100;, score=0.449 total time=   1.1s\n",
      "[CV 1/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=0.481 total time=   2.5s\n",
      "[CV 2/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=0.409 total time=   3.0s\n",
      "[CV 3/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=0.506 total time=   2.2s\n",
      "[CV 4/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=0.505 total time=   2.5s\n",
      "[CV 5/5] END max_depth=None, min_samples_split=10, n_estimators=200;, score=0.423 total time=   3.3s\n",
      "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=50;, score=0.470 total time=   0.6s\n",
      "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=50;, score=0.473 total time=   0.5s\n",
      "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=50;, score=0.482 total time=   0.6s\n",
      "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=50;, score=0.447 total time=   0.5s\n",
      "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=50;, score=0.483 total time=   0.5s\n",
      "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.489 total time=   1.2s\n",
      "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.460 total time=   0.9s\n",
      "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.512 total time=   0.9s\n",
      "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.466 total time=   0.9s\n",
      "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=100;, score=0.466 total time=   0.9s\n",
      "[CV 1/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.489 total time=   1.9s\n",
      "[CV 2/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.484 total time=   2.2s\n",
      "[CV 3/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.467 total time=   2.3s\n",
      "[CV 4/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.443 total time=   2.8s\n",
      "[CV 5/5] END max_depth=10, min_samples_split=2, n_estimators=200;, score=0.455 total time=   2.1s\n",
      "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=50;, score=0.447 total time=   0.4s\n",
      "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=50;, score=0.490 total time=   0.4s\n",
      "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=50;, score=0.474 total time=   0.5s\n",
      "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=50;, score=0.539 total time=   0.4s\n",
      "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=50;, score=0.425 total time=   0.4s\n",
      "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.481 total time=   1.1s\n",
      "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.411 total time=   1.2s\n",
      "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.490 total time=   1.0s\n",
      "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.494 total time=   1.1s\n",
      "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=100;, score=0.446 total time=   1.1s\n",
      "[CV 1/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.459 total time=   2.6s\n",
      "[CV 2/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.464 total time=   2.6s\n",
      "[CV 3/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.474 total time=   2.1s\n",
      "[CV 4/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.494 total time=   2.2s\n",
      "[CV 5/5] END max_depth=10, min_samples_split=5, n_estimators=200;, score=0.432 total time=   2.5s\n",
      "[CV 1/5] END max_depth=10, min_samples_split=10, n_estimators=50;, score=0.472 total time=   0.6s\n",
      "[CV 2/5] END max_depth=10, min_samples_split=10, n_estimators=50;, score=0.403 total time=   0.5s\n",
      "[CV 3/5] END max_depth=10, min_samples_split=10, n_estimators=50;, score=0.525 total time=   0.5s\n",
      "[CV 4/5] END max_depth=10, min_samples_split=10, n_estimators=50;, score=0.506 total time=   0.5s\n",
      "[CV 5/5] END max_depth=10, min_samples_split=10, n_estimators=50;, score=0.459 total time=   0.4s\n",
      "[CV 1/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=0.480 total time=   1.0s\n",
      "[CV 2/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=0.424 total time=   1.0s\n",
      "[CV 3/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=0.487 total time=   1.1s\n",
      "[CV 4/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=0.524 total time=   1.2s\n",
      "[CV 5/5] END max_depth=10, min_samples_split=10, n_estimators=100;, score=0.451 total time=   1.0s\n",
      "[CV 1/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=0.449 total time=   2.1s\n",
      "[CV 2/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=0.409 total time=   2.5s\n",
      "[CV 3/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=0.497 total time=   2.4s\n",
      "[CV 4/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=0.475 total time=   3.1s\n",
      "[CV 5/5] END max_depth=10, min_samples_split=10, n_estimators=200;, score=0.423 total time=   3.4s\n",
      "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=50;, score=0.478 total time=   0.9s\n",
      "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=50;, score=0.454 total time=   0.8s\n",
      "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=50;, score=0.440 total time=   0.6s\n",
      "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=50;, score=0.442 total time=   0.5s\n",
      "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=50;, score=0.489 total time=   0.5s\n",
      "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.473 total time=   1.1s\n",
      "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.418 total time=   1.2s\n",
      "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.483 total time=   1.3s\n",
      "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.469 total time=   1.2s\n",
      "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=100;, score=0.485 total time=   1.5s\n",
      "[CV 1/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=0.494 total time=   2.6s\n",
      "[CV 2/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=0.471 total time=   2.1s\n",
      "[CV 3/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=0.495 total time=   2.0s\n",
      "[CV 4/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=0.469 total time=   2.2s\n",
      "[CV 5/5] END max_depth=20, min_samples_split=2, n_estimators=200;, score=0.474 total time=   2.6s\n",
      "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=50;, score=0.427 total time=   0.7s\n",
      "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=50;, score=0.497 total time=   0.7s\n",
      "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=50;, score=0.453 total time=   0.5s\n",
      "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=50;, score=0.525 total time=   0.6s\n",
      "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=50;, score=0.435 total time=   0.7s\n",
      "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.487 total time=   1.0s\n",
      "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.517 total time=   1.0s\n",
      "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.491 total time=   0.9s\n",
      "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.519 total time=   1.1s\n",
      "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=100;, score=0.468 total time=   1.0s\n",
      "[CV 1/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=0.472 total time=   2.3s\n",
      "[CV 2/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=0.490 total time=   2.4s\n",
      "[CV 3/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=0.487 total time=   2.2s\n",
      "[CV 4/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=0.534 total time=   2.0s\n",
      "[CV 5/5] END max_depth=20, min_samples_split=5, n_estimators=200;, score=0.429 total time=   2.2s\n",
      "[CV 1/5] END max_depth=20, min_samples_split=10, n_estimators=50;, score=0.504 total time=   0.4s\n",
      "[CV 2/5] END max_depth=20, min_samples_split=10, n_estimators=50;, score=0.409 total time=   0.4s\n",
      "[CV 3/5] END max_depth=20, min_samples_split=10, n_estimators=50;, score=0.502 total time=   0.4s\n",
      "[CV 4/5] END max_depth=20, min_samples_split=10, n_estimators=50;, score=0.498 total time=   0.4s\n",
      "[CV 5/5] END max_depth=20, min_samples_split=10, n_estimators=50;, score=0.437 total time=   0.4s\n",
      "[CV 1/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=0.453 total time=   0.9s\n",
      "[CV 2/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=0.406 total time=   0.9s\n",
      "[CV 3/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=0.507 total time=   0.9s\n",
      "[CV 4/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=0.571 total time=   1.1s\n",
      "[CV 5/5] END max_depth=20, min_samples_split=10, n_estimators=100;, score=0.449 total time=   1.0s\n",
      "[CV 1/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=0.481 total time=   2.3s\n",
      "[CV 2/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=0.409 total time=   2.0s\n",
      "[CV 3/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=0.506 total time=   1.9s\n",
      "[CV 4/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=0.505 total time=   1.8s\n",
      "[CV 5/5] END max_depth=20, min_samples_split=10, n_estimators=200;, score=0.423 total time=   3.8s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=50;, score=0.478 total time=   1.2s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=50;, score=0.454 total time=   1.6s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=50;, score=0.440 total time=   1.5s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=50;, score=0.442 total time=   0.9s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=50;, score=0.489 total time=   1.0s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=0.473 total time=   1.9s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=0.418 total time=   1.4s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=0.483 total time=   1.6s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=0.469 total time=   1.6s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=100;, score=0.485 total time=   1.5s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=0.494 total time=   3.7s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=0.471 total time=   2.8s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=0.510 total time=   3.9s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=0.469 total time=   6.7s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=2, n_estimators=200;, score=0.474 total time=   3.9s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=5, n_estimators=50;, score=0.427 total time=   0.5s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=5, n_estimators=50;, score=0.497 total time=   0.5s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=5, n_estimators=50;, score=0.453 total time=   0.4s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=5, n_estimators=50;, score=0.525 total time=   0.4s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=5, n_estimators=50;, score=0.435 total time=   0.4s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=0.487 total time=   1.0s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=0.517 total time=   1.1s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=0.491 total time=   0.9s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=0.519 total time=   0.9s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=5, n_estimators=100;, score=0.468 total time=   0.9s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=0.472 total time=   2.0s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=0.490 total time=   1.8s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=0.487 total time=   2.2s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=0.534 total time=   2.3s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=5, n_estimators=200;, score=0.429 total time=   2.3s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=10, n_estimators=50;, score=0.504 total time=   0.5s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=10, n_estimators=50;, score=0.409 total time=   0.5s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=10, n_estimators=50;, score=0.502 total time=   0.7s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=10, n_estimators=50;, score=0.498 total time=   0.4s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=10, n_estimators=50;, score=0.437 total time=   0.4s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=0.453 total time=   0.8s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=0.406 total time=   0.9s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=0.507 total time=   0.8s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=0.571 total time=   1.0s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=10, n_estimators=100;, score=0.449 total time=   0.9s\n",
      "[CV 1/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=0.481 total time=   2.1s\n",
      "[CV 2/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=0.409 total time=   2.5s\n",
      "[CV 3/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=0.506 total time=   2.3s\n",
      "[CV 4/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=0.505 total time=   2.2s\n",
      "[CV 5/5] END max_depth=30, min_samples_split=10, n_estimators=200;, score=0.423 total time=   2.0s\n",
      "F1 Score: 0.4605 for Params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "F1 Score: 0.4657 for Params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.4836 for Params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "F1 Score: 0.4674 for Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "F1 Score: 0.4965 for Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.4823 for Params: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "F1 Score: 0.4701 for Params: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "F1 Score: 0.4772 for Params: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "F1 Score: 0.4650 for Params: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "F1 Score: 0.4712 for Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "F1 Score: 0.4786 for Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.4675 for Params: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "F1 Score: 0.4749 for Params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "F1 Score: 0.4644 for Params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.4647 for Params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "F1 Score: 0.4727 for Params: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "F1 Score: 0.4730 for Params: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "F1 Score: 0.4509 for Params: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "F1 Score: 0.4605 for Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "F1 Score: 0.4657 for Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.4806 for Params: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "F1 Score: 0.4674 for Params: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "F1 Score: 0.4965 for Params: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.4823 for Params: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "F1 Score: 0.4701 for Params: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "F1 Score: 0.4772 for Params: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "F1 Score: 0.4650 for Params: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "F1 Score: 0.4605 for Params: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "F1 Score: 0.4657 for Params: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "F1 Score: 0.4836 for Params: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "F1 Score: 0.4674 for Params: {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "F1 Score: 0.4965 for Params: {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.4823 for Params: {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "F1 Score: 0.4701 for Params: {'max_depth': 30, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "F1 Score: 0.4772 for Params: {'max_depth': 30, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "F1 Score: 0.4650 for Params: {'max_depth': 30, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best model parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 Score: 0.4431232296125913\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameters grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "# Create a scorer using F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, scoring=f1_scorer, cv=5, verbose=3)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "    print(f\"F1 Score: {mean_score:.4f} for Params: {params}\")\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Optionally decode the predictions back to original string labels\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a1637a-5898-49e8-924b-b88f899a86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "879248b8-bcd9-4729-9c1a-252641eb80a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Example DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'embedding': [list(np.random.rand(100)) for _ in range(500)],\n",
    "#     'label': ['label1', 'label2', 'label3', 'label4'] * 125\n",
    "# })\n",
    "\n",
    "# Convert the DataFrame\n",
    "embeddings = np.array(df['embedding'].tolist())\n",
    "labels = df['label']\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define weighted F1 score as a custom metric\n",
    "def weighted_f1_score(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    return f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with weighted F1 score as a metric\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[weighted_f1_score])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_weighted_f1_score', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "_, weighted_f1 = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Weighted F1 Score on Test Set: {weighted_f1:.4f}\")\n",
    "\n",
    "# Optionally, make predictions and decode labels\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee4426-037e-4c87-824b-5b89b09f252f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17be0e3-2118-4242-bf4d-1647df58f13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdddbd16-ea6d-4023-9a77-00e313a92a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d3d11-b9bb-4395-af7b-596c12aaf7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
